{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22142d03-67ac-4dab-ba27-9f22690244d5",
   "metadata": {},
   "source": [
    "# ä¸Šä¸‹æ–‡å·¥ç¨‹\n",
    "\n",
    "[ä¸Šä¸‹æ–‡å·¥ç¨‹](https://docs.langchain.com/oss/python/langchain/context-engineering)ï¼ˆContext Engineeringï¼‰å¯¹äº Agent å¾—å‡ºæ­£ç¡®çš„ç»“æœè‡³å…³é‡è¦ã€‚æ¨¡å‹å›ç­”ä¸å¥½ï¼Œå¾ˆå¤šæ—¶å€™ä¸æ˜¯å› ä¸ºèƒ½åŠ›ä¸è¶³ï¼Œè€Œæ˜¯å› ä¸ºæ²¡æœ‰è·å¾—è¶³ä»¥æ¨æ–­å‡ºæ­£ç¡®ç»“æœçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼Œå¢å¼º Agent è·å–å’Œç®¡ç†ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œæ˜¯å¾ˆæœ‰å¿…è¦çš„ã€‚\n",
    "\n",
    "**LangGraph å°†ä¸Šä¸‹æ–‡åˆ†ä¸ºä¸‰ç§ç±»å‹ï¼š**\n",
    "\n",
    "- æ¨¡å‹ä¸Šä¸‹æ–‡ï¼ˆModel Contextï¼‰\n",
    "- å·¥å…·ä¸Šä¸‹æ–‡ï¼ˆTool Contextï¼‰\n",
    "- ç”Ÿå‘½å‘¨æœŸä¸Šä¸‹æ–‡ï¼ˆLife-cycle Contextï¼‰\n",
    "\n",
    "æ— è®ºå“ªç§ Contextï¼Œéƒ½éœ€è¦å®šä¹‰å®ƒçš„ Schemaã€‚åœ¨è¿™æ–¹é¢ï¼ŒLangGraph æä¾›äº†ç›¸å½“é«˜çš„è‡ªç”±åº¦ï¼Œä½ å¯ä»¥ä½¿ç”¨ `dataclasses`ã€`pydantic`ã€`TypedDict` è¿™äº›åŒ…çš„ä»»æ„ä¸€ä¸ªåˆ›å»ºä½ çš„ Context Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4412b776-1f74-41e8-9444-f1724bcff581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a92bc3-bd5e-4ad9-8b4a-2ae1d0f171cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "\n",
    "from typing import Callable\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, wrap_model_call, ModelRequest, ModelResponse, SummarizationMiddleware\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.sqlite import SqliteStore\n",
    "\n",
    "# åŠ è½½æ¨¡å‹é…ç½®\n",
    "_ = load_dotenv()\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-coder-plus\",\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f5631-a9f6-40d5-9e57-d652e347c993",
   "metadata": {},
   "source": [
    "## ä¸€ã€åŠ¨æ€ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯\n",
    "\n",
    "ä¸Šä¸‹æ–‡å·¥ç¨‹ä¸å‰åºç« èŠ‚çš„ä¸­é—´ä»¶ï¼ˆmiddlewareï¼‰å’Œè®°å¿†ï¼ˆmemoryï¼‰å¯†ä¸å¯åˆ†ã€‚ä¸Šä¸‹æ–‡çš„å…·ä½“å®ç°ä¾èµ–ä¸­é—´ä»¶ï¼Œè€Œä¸Šä¸‹æ–‡çš„å­˜å‚¨åˆ™ä¾èµ–è®°å¿†ç³»ç»Ÿã€‚å…·ä½“æ¥è®²ï¼ŒLangGraph é¢„ç½®äº† `@dynamic_prompt` ä¸­é—´ä»¶ï¼Œç”¨äºåŠ¨æ€ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯ã€‚\n",
    "\n",
    "æ—¢ç„¶æ˜¯åŠ¨æ€ä¿®æ”¹ï¼Œè‚¯å®šéœ€è¦æŸä¸ªæ¡ä»¶æ¥è§¦å‘ä¿®æ”¹ã€‚é™¤äº†å¼€å‘è§¦å‘é€»è¾‘ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä»æ™ºèƒ½ä½“ä¸­è·å–è§¦å‘é€»è¾‘æ‰€éœ€çš„å³æ—¶å˜é‡ã€‚è¿™äº›å˜é‡é€šå¸¸å­˜å‚¨åœ¨ä»¥ä¸‹ä¸‰ä¸ªå­˜å‚¨ä»‹è´¨ä¸­ï¼š\n",
    "\n",
    "- è¿è¡Œæ—¶ï¼ˆRuntimeï¼‰- æ‰€æœ‰èŠ‚ç‚¹å…±äº«ä¸€ä¸ª Runtimeã€‚åŒä¸€æ—¶åˆ»ï¼Œæ‰€æœ‰èŠ‚ç‚¹å–åˆ°çš„ Runtime çš„å€¼æ˜¯ç›¸åŒçš„ã€‚ä¸€èˆ¬ç”¨äºå­˜å‚¨æ—¶æ•ˆæ€§è¦æ±‚è¾ƒé«˜çš„ä¿¡æ¯ã€‚\n",
    "- çŸ­æœŸè®°å¿†ï¼ˆStateï¼‰- åœ¨èŠ‚ç‚¹ä¹‹é—´æŒ‰é¡ºåºä¼ é€’ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶ä¸Šä¸€ä¸ªèŠ‚ç‚¹å¤„ç†åçš„ Stateã€‚ä¸»è¦ç”¨äºå­˜å‚¨ Prompt å’Œ AI Messageã€‚\n",
    "- é•¿æœŸè®°å¿†ï¼ˆStoreï¼‰- è´Ÿè´£æŒä¹…åŒ–å­˜å‚¨ï¼Œå¯ä»¥è·¨ Workflow / Agent ä¿å­˜ä¿¡æ¯ã€‚å¯ä»¥ç”¨æ¥å­˜ç”¨æˆ·åå¥½ã€ä»¥å‰ç®—è¿‡çš„ç»Ÿè®¡å€¼ç­‰ã€‚\n",
    "\n",
    "ä»¥ä¸‹ä¸‰ä¸ªä¾‹å­ï¼Œåˆ†åˆ«æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ¥è‡ª Runtimeã€Stateã€Store ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œç¼–å†™è§¦å‘æ¡ä»¶ã€‚\n",
    "\n",
    "### 1ï¼‰ä½¿ç”¨ `State` ç®¡ç†ä¸Šä¸‹æ–‡\n",
    "\n",
    "åˆ©ç”¨ `State` ä¸­è•´å«çš„ä¿¡æ¯æ“çºµ system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9eb1bf-37b9-4aab-a8f3-0e1cffca6eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n",
      "This is a long conversation - be extra concise.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å¹¿å·å¤©æ°”å¾ˆå¥½\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "åƒç‚¹ä»€ä¹ˆå¥½å‘¢\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "è¦ä¸è¦åƒé¦™èŒ…é³—é±¼ç…²\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "é¦™èŒ…æ˜¯ä»€ä¹ˆ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "é¦™èŒ…åˆåæŸ æª¬è‰ï¼Œå¸¸è§äºæ³°å¼å†¬é˜´åŠŸæ±¤ã€è¶Šå—çƒ¤è‚‰\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "auv é‚£è¿˜ç­‰ä»€ä¹ˆï¼Œå’±åƒå»å§\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "èµ°å˜ï¼ï¼ˆæ‘¸å‡ºæ‰‹æœºæ‰“å¼€å¯¼èˆªï¼‰å¤©æ²³åŒ—é‚£å®¶æ³°é¤è€åº—èµ°èµ·ï½ä½ è¯·å®¢å“ˆï¼ˆçœ¨çœ¼ï¼‰\n"
     ]
    }
   ],
   "source": [
    "@dynamic_prompt\n",
    "def state_aware_prompt(request: ModelRequest) -> str:\n",
    "    # request.messages is a shortcut for request.state[\"messages\"]\n",
    "    message_count = len(request.messages)\n",
    "\n",
    "    base = \"You are a helpful assistant.\"\n",
    "\n",
    "    if message_count > 6:\n",
    "        base += \"\\nThis is a long conversation - be extra concise.\"\n",
    "\n",
    "    # ä¸´æ—¶æ‰“å°baseçœ‹æ•ˆæœ\n",
    "    print(base)\n",
    "\n",
    "    return base\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    middleware=[state_aware_prompt]\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"å¹¿å·å¤©æ°”å¾ˆå¥½\"},\n",
    "        {\"role\": \"user\", \"content\": \"åƒç‚¹ä»€ä¹ˆå¥½å‘¢\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"è¦ä¸è¦åƒé¦™èŒ…é³—é±¼ç…²\"},\n",
    "        {\"role\": \"user\", \"content\": \"é¦™èŒ…æ˜¯ä»€ä¹ˆ\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"é¦™èŒ…åˆåæŸ æª¬è‰ï¼Œå¸¸è§äºæ³°å¼å†¬é˜´åŠŸæ±¤ã€è¶Šå—çƒ¤è‚‰\"},\n",
    "        {\"role\": \"user\", \"content\": \"auv é‚£è¿˜ç­‰ä»€ä¹ˆï¼Œå’±åƒå»å§\"},\n",
    "    ]},\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c9a5d-90a9-464d-b0dd-bc0b383ce149",
   "metadata": {},
   "source": [
    "æŠŠ `message_count > 6` é‡Œçš„ 6 æ”¹æˆ 7ï¼Œè¯•è¯•çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4c6d6-cea7-448a-ae42-33bcc64b77a3",
   "metadata": {},
   "source": [
    "### 2ï¼‰ä½¿ç”¨ `Store` ç®¡ç†ä¸Šä¸‹æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98294b9-7a56-499b-a190-e0181e2d6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Context:\n",
    "    user_id: str\n",
    "\n",
    "@dynamic_prompt\n",
    "def store_aware_prompt(request: ModelRequest) -> str:\n",
    "    user_id = request.runtime.context.user_id\n",
    "\n",
    "    # Read from Store: get user preferences\n",
    "    store = request.runtime.store\n",
    "    user_prefs = store.get((\"preferences\",), user_id)\n",
    "\n",
    "    base = \"You are a helpful assistant.\"\n",
    "\n",
    "    if user_prefs:\n",
    "        style = user_prefs.value.get(\"communication_style\", \"balanced\")\n",
    "        base += f\"\\nUser prefers {style} responses.\"\n",
    "\n",
    "    return base\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    middleware=[store_aware_prompt],\n",
    "    context_schema=Context,\n",
    "    store=store,\n",
    ")\n",
    "\n",
    "# é¢„ç½®ä¸¤æ¡åå¥½ä¿¡æ¯\n",
    "store.put((\"preferences\",), \"user_1\", {\"communication_style\": \"Chinese\"})\n",
    "store.put((\"preferences\",), \"user_2\", {\"communication_style\": \"Korean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509b7bc4-1a3b-4b32-9afe-3db4894c1ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant. Please be extra concise.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is a \"hold short line\"?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Hold short line**ï¼ˆç­‰å¾…çº¿/åœæ­¢çº¿ï¼‰æ˜¯æœºåœºè·‘é“æˆ–æ»‘è¡Œé“ä¸Šçš„æ ‡è®°çº¿ï¼ŒæŒ‡ç¤ºé£æœºå¿…é¡»åœ¨æ­¤å¤„åœä¸‹ç­‰å¾…å¡”å°è¿›ä¸€æ­¥æŒ‡ä»¤ã€‚\n",
      "\n",
      "- **ä½œç”¨**ï¼šé˜²æ­¢é£æœºè¯¯å…¥æ´»åŠ¨è·‘é“ï¼Œç¡®ä¿ç©ºä¸­äº¤é€šç®¡åˆ¶çš„å®‰å…¨é—´éš”\n",
      "- **ä½ç½®**ï¼šé€šå¸¸ä½äºè·‘é“å…¥å£å‰çš„æ»‘è¡Œé“ä¸Š\n",
      "- **æ ‡å¿—**ï¼šç”±å››æ¡é»„è‰²å®çº¿ç»„æˆï¼ˆä¸¤å®ä¸¤è™šï¼‰\n",
      "- **ç¨‹åº**ï¼šé£è¡Œå‘˜éœ€åœ¨æ­¤çº¿å‰åœä¸‹ï¼Œç­‰å¾…ATCè®¸å¯åæ‰èƒ½ç»§ç»­æ»‘è¡Œæˆ–èµ·é£\n",
      "\n",
      "è¿™æ˜¯æœºåœºåœ°é¢è¿è¡Œçš„é‡è¦å®‰å…¨æªæ–½ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ç”¨æˆ·1å–œæ¬¢ä¸­æ–‡å›å¤\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please be extra concise.\"},\n",
    "        {\"role\": \"user\", \"content\": 'What is a \"hold short line\"?'}\n",
    "    ]},\n",
    "    context=Context(user_id=\"user_1\"),\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288d2a65-b328-4448-a155-222afc84f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant. Please be extra concise.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is a \"hold short line\"?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "'Hold short line'ì€ ê³µí•­ í™œì£¼ë¡œì—ì„œ í•­ê³µê¸°ê°€ ì´ë¥™ ëŒ€ê¸° ì‹œ ë©ˆì¶°ì•¼ í•˜ëŠ” ìœ„ì¹˜ë¥¼ í‘œì‹œí•˜ëŠ” ì„ ì…ë‹ˆë‹¤. í•­ê³µê¸°ëŠ” ì´ ì„  ë’¤ì—ì„œ ëŒ€ê¸°í•´ì•¼ í•˜ë©°, êµí†µê´€ì œì†Œì˜ í—ˆë½ ì—†ì´ëŠ” ë„˜ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•ˆì „í•œ ê°„ê²© ìœ ì§€ì™€ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ç”¨æˆ·2å–œæ¬¢éŸ©æ–‡å›å¤\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please be extra concise.\"},\n",
    "        {\"role\": \"user\", \"content\": 'What is a \"hold short line\"?'}\n",
    "    ]},\n",
    "    context=Context(user_id=\"user_2\"),\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea07a9a-289f-4f4c-987c-a76f4818e753",
   "metadata": {},
   "source": [
    "### 3ï¼‰ä½¿ç”¨ `Runtime` ç®¡ç†ä¸Šä¸‹æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8776b008-b2a7-4947-8bed-f51460d37712",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Context:\n",
    "    user_role: str\n",
    "    deployment_env: str\n",
    "\n",
    "@dynamic_prompt\n",
    "def context_aware_prompt(request: ModelRequest) -> str:\n",
    "    # Read from Runtime Context: user role and environment\n",
    "    user_role = request.runtime.context.user_role\n",
    "    env = request.runtime.context.deployment_env\n",
    "\n",
    "    base = \"You are a helpful assistant.\"\n",
    "\n",
    "    if user_role == \"admin\":\n",
    "        base += \"\\nYou can use the get_weather tool.\"\n",
    "    else:\n",
    "        base += \"\\nYou are prohibited from using the get_weather tool.\"\n",
    "\n",
    "    if env == \"production\":\n",
    "        base += \"\\nBe extra careful with any data modifications.\"\n",
    "\n",
    "    return base\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather],\n",
    "    middleware=[context_aware_prompt],\n",
    "    context_schema=Context,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1a91c1-f87f-46ce-b6fe-251040867390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_bb6b7418905a42ed9a0e5681)\n",
      " Call ID: call_bb6b7418905a42ed9a0e5681\n",
      "  Args:\n",
      "    city: å¹¿å·\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "It's always sunny in å¹¿å·!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æ ¹æ®å·¥å…·è¿”å›çš„ä¿¡æ¯ï¼Œå¹¿å·ä»Šå¤©çš„å¤©æ°”æ€»æ˜¯æ™´å¤©ï¼ä¸è¿‡ï¼Œè¿™ä¸ªä¿¡æ¯å¯èƒ½æ˜¯ä¸€ä¸ªé»˜è®¤å›å¤ï¼Œå®é™…çš„å¤©æ°”æƒ…å†µå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚å»ºè®®æ‚¨æŸ¥çœ‹å®æ—¶å¤©æ°”é¢„æŠ¥ä»¥è·å–å‡†ç¡®çš„ä¿¡æ¯ã€‚\n"
     ]
    }
   ],
   "source": [
    "# åˆ©ç”¨ Runtime ä¸­çš„ä¸¤ä¸ªå˜é‡ï¼ŒåŠ¨æ€æ§åˆ¶ System prompt\n",
    "# å°† user_role è®¾ä¸º adminï¼Œå…è®¸ä½¿ç”¨å¤©æ°”æŸ¥è¯¢å·¥å…·\n",
    "config = {'configurable': {'thread_id': str(uuid.uuid4())}}\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"}]},\n",
    "    context=Context(user_role=\"admin\", deployment_env=\"production\"),\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51ba924-4b11-4268-a498-29104a3916bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_e85c19f2a8d944fda7a02f07)\n",
      " Call ID: call_e85c19f2a8d944fda7a02f07\n",
      "  Args:\n",
      "    city: å¹¿å·\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "It's always sunny in å¹¿å·!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å¹¿å·ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œé˜³å…‰æ˜åªšï¼å¦‚æœæ‚¨åœ¨å¹¿å·ï¼Œè®°å¾—åšå¥½é˜²æ™’æªæ–½ï¼Œäº«å—è¿™ç¾å¥½çš„ä¸€å¤©å§ï¼ğŸŒ\n"
     ]
    }
   ],
   "source": [
    "# è‹¥å°† user_role æ”¹ä¸º viewerï¼Œåˆ™æ— æ³•ä½¿ç”¨å¤©æ°”æŸ¥è¯¢å·¥å…·\n",
    "config = {'configurable': {'thread_id': str(uuid.uuid4())}}\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"}]},\n",
    "    context=Context(user_role=\"viewer\", deployment_env=\"production\"),\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae08b79-0e58-4d4a-90c9-22e9624a4a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ', additional_kwargs={}, response_metadata={}, id='6a5d0cac-d25d-445a-940a-fdfed01e5868'),\n",
       " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 287, 'total_tokens': 308, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen3-coder-plus', 'system_fingerprint': None, 'id': 'chatcmpl-6521559f-5ac6-9657-99a6-a58f36c2204f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c2912-47f3-7293-87a7-516e5428b24c-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'å¹¿å·'}, 'id': 'call_e85c19f2a8d944fda7a02f07', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 287, 'output_tokens': 21, 'total_tokens': 308, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}),\n",
       " ToolMessage(content=\"It's always sunny in å¹¿å·!\", name='get_weather', id='2a9472af-23cd-419b-873e-226c2f0dd22e', tool_call_id='call_e85c19f2a8d944fda7a02f07'),\n",
       " AIMessage(content='å¹¿å·ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œé˜³å…‰æ˜åªšï¼å¦‚æœæ‚¨åœ¨å¹¿å·ï¼Œè®°å¾—åšå¥½é˜²æ™’æªæ–½ï¼Œäº«å—è¿™ç¾å¥½çš„ä¸€å¤©å§ï¼ğŸŒ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 331, 'total_tokens': 355, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 192}}, 'model_provider': 'openai', 'model_name': 'qwen3-coder-plus', 'system_fingerprint': None, 'id': 'chatcmpl-0fae5d97-2a50-9de2-9f10-99039c83f289', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c2912-4a93-7f02-9db4-9455ead87e42-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 331, 'output_tokens': 24, 'total_tokens': 355, 'input_token_details': {'cache_read': 192}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e6d02-152c-4a13-a5fc-60e27113d596",
   "metadata": {},
   "source": [
    "## äºŒã€åŠ¨æ€ä¿®æ”¹æ¶ˆæ¯åˆ—è¡¨\n",
    "\n",
    "LangGraph é¢„åˆ¶äº†åŠ¨æ€ä¿®æ”¹æ¶ˆæ¯åˆ—è¡¨ï¼ˆMessagesï¼‰çš„ä¸­é—´ä»¶ `@wrap_model_call`ã€‚ä¸Šä¸€èŠ‚å·²ç»æ¼”ç¤ºå¦‚ä½•ä» `State`ã€`Store`ã€`Runtime` ä¸­è·å–ä¸Šä¸‹æ–‡ï¼Œæœ¬èŠ‚å°†ä¸å†ä¸€ä¸€æ¼”ç¤ºã€‚åœ¨ä¸‹é¢è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ `Runtime` å°†æœ¬åœ°æ–‡ä»¶çš„å†…å®¹æ³¨å…¥æ¶ˆæ¯åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc7b4e4a-1dca-4e40-bafa-840373986c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FileContext:\n",
    "    uploaded_files: list[dict]\n",
    "\n",
    "@wrap_model_call\n",
    "def inject_file_context(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse]\n",
    ") -> ModelResponse:\n",
    "    \"\"\"Inject context about files user has uploaded this session.\"\"\"\n",
    "    uploaded_files = request.runtime.context.uploaded_files\n",
    "\n",
    "    try:\n",
    "        base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except Exception as e:\n",
    "        import ipynbname\n",
    "        import os\n",
    "        notebook_path = ipynbname.path()\n",
    "        base_dir = os.path.dirname(notebook_path)\n",
    "\n",
    "    file_sections = []\n",
    "    for file in uploaded_files:\n",
    "        name, ftype = \"\", \"\"\n",
    "        path = file.get(\"path\")\n",
    "        if path:\n",
    "            base_filename = os.path.basename(path)\n",
    "            stem, ext = os.path.splitext(base_filename)\n",
    "            name = stem or base_filename\n",
    "            ftype = (ext.lstrip(\".\") if ext else None)\n",
    "\n",
    "            # æ„å»ºæ–‡ä»¶æè¿°å†…å®¹\n",
    "            content_list = [f\"åç§°: {name}\"]\n",
    "            if ftype:\n",
    "                content_list.append(f\"ç±»å‹: {ftype}\")\n",
    "\n",
    "            # è§£æç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„\n",
    "            abs_path = path if os.path.isabs(path) else os.path.join(base_dir, path)\n",
    "\n",
    "            # è¯»å–æ–‡ä»¶å†…å®¹\n",
    "            content_block = \"\"\n",
    "            if abs_path and os.path.exists(abs_path):\n",
    "                try:\n",
    "                    with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content_block = f.read()\n",
    "                except Exception as e:\n",
    "                    content_block = f\"[è¯»å–æ–‡ä»¶é”™è¯¯ '{abs_path}': {e}]\"\n",
    "            else:\n",
    "                content_block = \"[æ–‡ä»¶è·¯å¾„ç¼ºå¤±æˆ–æœªæ‰¾åˆ°]\"\n",
    "\n",
    "            section = (\n",
    "                f\"---\\n\"\n",
    "                f\"{chr(10).join(content_list)}\\n\\n\"\n",
    "                f\"{content_block}\\n\"\n",
    "                f\"---\"\n",
    "            )\n",
    "            file_sections.append(section)\n",
    "\n",
    "        file_context = (\n",
    "            \"å·²åŠ è½½çš„ä¼šè¯æ–‡ä»¶ï¼š\\n\"\n",
    "            f\"{chr(10).join(file_sections)}\"\n",
    "            \"\\nå›ç­”é—®é¢˜æ—¶è¯·å‚è€ƒè¿™äº›æ–‡ä»¶ã€‚\"\n",
    "        )\n",
    "\n",
    "        # Inject file context before recent messages\n",
    "        messages = [  \n",
    "            *request.messages,\n",
    "            {\"role\": \"user\", \"content\": file_context},\n",
    "        ]\n",
    "        request = request.override(messages=messages)  \n",
    "\n",
    "    return handler(request)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    middleware=[inject_file_context],\n",
    "    context_schema=FileContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9048a7e0-2131-4841-82f2-b1f7fca1c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "å…³äºä¸Šæµ·åœ°é“çš„æ— è„¸ä¹˜å®¢ï¼Œæœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æ ¹æ®ã€Šä¸Šæµ·è§„åˆ™æ€ªè°ˆã€‹ä¸­çš„è§„åˆ™ï¼Œå…³äºä¸Šæµ·åœ°é“çš„æ— è„¸ä¹˜å®¢ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
      "\n",
      "**å…³é”®è§„åˆ™ï¼šåœ°é“ä¸Šçš„é™Œç”Ÿäºº**\n",
      "- **å‡ºç°æ¡ä»¶**ï¼šåœ°é“è¿è¥ç»“æŸåï¼Œè‹¥ä½ ä»èº«å¤„è½¦å¢å†…\n",
      "- **åº”å¯¹æ–¹å¼**ï¼šå½“æ— è„¸ä¹˜å®¢é—®\"ä½ è¦å»å“ªï¼Ÿ\"æ—¶ï¼Œ**åªèƒ½æŠ¥ä¸€ä¸ªçœŸå®å­˜åœ¨çš„ä¸Šæµ·åœ°å**\n",
      "- **ç¦å¿Œè¡Œä¸º**ï¼šè¯´å‡ºä¸å­˜åœ¨çš„åœ°åæˆ–ä¿æŒæ²‰é»˜\n",
      "- **ä¸¥é‡åæœ**ï¼šå¦‚è¿åï¼Œä½ ä¼šåœ¨è½¦å¢å†…çœ‹åˆ°è‡ªå·±çš„å°¸ä½“\n",
      "\n",
      "å› æ­¤ï¼Œå¦‚æœé‡åˆ°åœ°é“æ— è„¸ä¹˜å®¢ï¼Œæœ€é‡è¦çš„æ˜¯ä¿æŒå†·é™ï¼Œå‡†ç¡®å›ç­”ä¸€ä¸ªçœŸå®ä¸Šæµ·åœ°åï¼Œç»ä¸èƒ½èƒ¡ä¹±ç¼–é€ åœ°ç‚¹æˆ–ä¸ä½œå›åº”ã€‚\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"å…³äºä¸Šæµ·åœ°é“çš„æ— è„¸ä¹˜å®¢ï¼Œæœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„ï¼Ÿ\",\n",
    "        }],\n",
    "    },\n",
    "    context=FileContext(uploaded_files=[{\"path\": \"./docs/rule_horror.md\"}]),\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59498b8d-9b5d-4cb3-bc86-0230e76a8121",
   "metadata": {},
   "source": [
    "## ä¸‰ã€åœ¨å·¥å…·ä¸­ä½¿ç”¨ä¸Šä¸‹æ–‡\n",
    "\n",
    "ä¸‹é¢ï¼Œæˆ‘ä»¬å°è¯•åœ¨å·¥å…·ä¸­ä½¿ç”¨å­˜å‚¨åœ¨ `SqliteStore` ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a33e3357-71e3-4ae3-a956-9e6ad0076a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ é™¤SQLiteæ•°æ®åº“\n",
    "if os.path.exists(\"user-info.db\"):\n",
    "    os.remove(\"user-info.db\")\n",
    "\n",
    "# åˆ›å»ºSQLiteå­˜å‚¨\n",
    "conn = sqlite3.connect(\"user-info.db\", check_same_thread=False, isolation_level=None)\n",
    "conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "conn.execute(\"PRAGMA busy_timeout = 30000;\")\n",
    "\n",
    "store = SqliteStore(conn)\n",
    "\n",
    "# é¢„ç½®ä¸¤æ¡ç”¨æˆ·ä¿¡æ¯\n",
    "store.put((\"user_info\",), \"æŸ³å¦‚çƒŸ\", {\"description\": \"æ¸…å†·æ‰å¥³ï¼Œèº«æ€€ç»æŠ€ï¼Œä¸ºå¯»èº«ä¸–ä¹‹è°œè¸å…¥æ±Ÿæ¹–ã€‚\", \"birthplace\": \"å´å…´å¿\"})\n",
    "store.put((\"user_info\",), \"è‹æ…•ç™½\", {\"description\": \"å­¤å‚²å‰‘å®¢ï¼Œå‰‘æ³•è¶…ç¾¤ï¼ŒèƒŒè´Ÿå®¶æ—è¡€ä»‡ï¼Œéšäºå¸‚äº•è¿½å¯»çœŸç›¸ã€‚\", \"birthplace\": \"æ­å¿\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd8c15-f80a-4885-9477-37d84bbf9495",
   "metadata": {},
   "source": [
    "### 1ï¼‰åŸºç¡€ç”¨ä¾‹\n",
    "\n",
    "ä½¿ç”¨ `ToolRuntime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de7547ce-963e-48bf-9855-ab32521c034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_user_data(\n",
    "    user_id: str,\n",
    "    runtime: ToolRuntime\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetch user information from the in-memory store.\n",
    "\n",
    "    :param user_id: The unique identifier of the user.\n",
    "    :param runtime: The tool runtime context injected by the framework.\n",
    "    :return: The user's description string if found; an empty string otherwise.\n",
    "    \"\"\"\n",
    "    store = runtime.store\n",
    "    user_info = store.get((\"user_info\",), user_id)\n",
    "\n",
    "    user_desc = \"\"\n",
    "    if user_info:\n",
    "        user_desc = user_info.value.get(\"description\", \"\")\n",
    "\n",
    "    return user_desc\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[fetch_user_data],\n",
    "    store=store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60289d99-0235-464c-837c-407065b0096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "äº”åˆ†é’Ÿä¹‹å†…ï¼Œæˆ‘è¦æŸ³å¦‚çƒŸçš„å…¨éƒ¨ä¿¡æ¯\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æŸ³å¦‚çƒŸçš„ä¿¡æ¯æˆ‘ç›®å‰æ— æ³•ç›´æ¥è·å–ï¼Œå› ä¸ºç³»ç»Ÿä¸­æ²¡æœ‰å…³äºâ€œæŸ³å¦‚çƒŸâ€çš„æ•°æ®å­˜å‚¨è®°å½•ã€‚å¦‚æœä½ æœ‰å…·ä½“çš„ç”¨æˆ·IDæˆ–å…¶ä»–ç›¸å…³ä¿¡æ¯ï¼Œå¯ä»¥æä¾›ç»™æˆ‘ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æŸ¥è¯¢ã€‚å¦åˆ™ï¼Œå¯èƒ½éœ€è¦å…¶ä»–é€”å¾„æ¥è·å–ä½ æ‰€éœ€çš„ä¿¡æ¯ã€‚è¯·ç¡®è®¤æ˜¯å¦æœ‰è¯¯æˆ–æä¾›æ›´å¤šç»†èŠ‚ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"äº”åˆ†é’Ÿä¹‹å†…ï¼Œæˆ‘è¦æŸ³å¦‚çƒŸçš„å…¨éƒ¨ä¿¡æ¯\"\n",
    "    }]\n",
    "})\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620c05d-aa96-4d3b-9dfd-9774c99e38f7",
   "metadata": {},
   "source": [
    "### 2ï¼‰å¤æ‚ä¸€ç‚¹çš„ä¾‹å­\n",
    "\n",
    "ä½¿ç”¨ `ToolRuntime[Context]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1950ba66-93e9-43d2-af5b-b048783287bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Context:\n",
    "    key: str\n",
    "\n",
    "@tool\n",
    "def fetch_user_data(\n",
    "    user_id: str,\n",
    "    runtime: ToolRuntime[Context]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetch user information from the in-memory store.\n",
    "\n",
    "    :param user_id: The unique identifier of the user.\n",
    "    :param runtime: The tool runtime context injected by the framework.\n",
    "    :return: The user's description string if found; an empty string otherwise.\n",
    "    \"\"\"\n",
    "    key = runtime.context.key\n",
    "\n",
    "    store = runtime.store\n",
    "    user_info = store.get((\"user_info\",), user_id)\n",
    "\n",
    "    user_desc = \"\"\n",
    "    if user_info:\n",
    "        user_desc = user_info.value.get(key, \"\")\n",
    "\n",
    "    return f\"{key}: {user_desc}\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[fetch_user_data],\n",
    "    store=store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd427af-d129-401c-a643-eea926af05d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "äº”åˆ†é’Ÿä¹‹å†…ï¼Œæˆ‘è¦æŸ³å¦‚çƒŸçš„å…¨éƒ¨ä¿¡æ¯\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æŸ³å¦‚çƒŸçš„ä¿¡æ¯æ— æ³•é€šè¿‡ç°æœ‰å·¥å…·è·å–ï¼Œå› ä¸ºç³»ç»Ÿä¸­åªæä¾›äº†æ ¹æ®ç”¨æˆ·IDæŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯çš„åŠŸèƒ½ï¼Œå¹¶ä¸”éœ€è¦å…·ä½“ç”¨æˆ·IDæ‰èƒ½è°ƒç”¨ã€‚è‹¥ä½ æœ‰æŸ³å¦‚çƒŸçš„ç”¨æˆ·IDï¼Œå¯å°è¯•æä¾›ç»™æˆ‘ä»¥è¿›ä¸€æ­¥æ“ä½œï¼›å¦åˆ™ï¼Œå¯èƒ½éœ€è¦å…¶ä»–é€”å¾„æˆ–å·¥å…·æ¥è·å–æ‰€éœ€ä¿¡æ¯ã€‚\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"äº”åˆ†é’Ÿä¹‹å†…ï¼Œæˆ‘è¦æŸ³å¦‚çƒŸçš„å…¨éƒ¨ä¿¡æ¯\"}]},\n",
    "    context=Context(key=\"birthplace\"),\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a892c0-3a74-47d4-aef2-dd7bf3270ead",
   "metadata": {},
   "source": [
    "### å››ã€å‹ç¼©ä¸Šä¸‹æ–‡\n",
    "\n",
    "LangChain æä¾›äº†å†…ç½®çš„ä¸­é—´ä»¶ `SummarizationMiddleware` ç”¨äºå‹ç¼©ä¸Šä¸‹æ–‡ã€‚è¯¥ä¸­é—´ä»¶ç»´æŠ¤çš„æ˜¯å…¸å‹çš„ **ç”Ÿå‘½å‘¨æœŸä¸Šä¸‹æ–‡**ï¼Œä¸ **æ¨¡å‹ä¸Šä¸‹æ–‡** å’Œ **å·¥å…·ä¸Šä¸‹æ–‡** çš„ç¬æ€æ›´æ–°ä¸åŒï¼Œç”Ÿå‘½å‘¨æœŸä¸Šä¸‹æ–‡ä¼šæŒç»­æ›´æ–°ï¼šæŒç»­å°†æ—§æ¶ˆæ¯æ›¿æ¢ä¸ºæ‘˜è¦ã€‚\n",
    "\n",
    "é™¤éä¸Šä¸‹æ–‡è¶…é•¿ï¼Œå¯¼è‡´æ¨¡å‹èƒ½åŠ›é™ä½ï¼Œå¦åˆ™ä¸éœ€è¦ä½¿ç”¨ `SummarizationMiddleware`ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè§¦å‘æ‘˜è¦çš„å€¼å¯ä»¥è®¾å¾—è¾ƒå¤§ã€‚æ¯”å¦‚ï¼š\n",
    "\n",
    "- `max_tokens_before_summary`: 3000\n",
    "- `messages_to_keep`: 20\n",
    "\n",
    "> å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºä¸Šä¸‹æ–‡è…åï¼ˆContext Rotï¼‰çš„ä¿¡æ¯ï¼ŒChroma å›¢é˜Ÿåœ¨ 2025 å¹´ 7 æœˆ 14 æ—¥å‘å¸ƒçš„ [*Context Rot: How Increasing Input Tokens Impacts LLM Performance*](https://research.trychroma.com/context-rot)ï¼Œç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†é•¿ä¸Šä¸‹æ–‡å¯¼è‡´æ¨¡å‹æ€§èƒ½é€€åŒ–çš„ç°è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c75a7d59-9edf-4a00-af8a-3a6d06d6f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºçŸ­æœŸè®°å¿†\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# åˆ›å»ºå¸¦å†…ç½®æ‘˜è¦ä¸­é—´ä»¶çš„Agent\n",
    "# ä¸ºäº†è®©é…ç½®èƒ½åœ¨æˆ‘ä»¬çš„ä¾‹å­é‡Œç”Ÿæ•ˆï¼Œè¿™é‡Œçš„è§¦å‘å€¼è®¾å¾—å¾ˆå°\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=llm,\n",
    "            trigger=('tokens', 40),  # Trigger summarization at 40 tokens\n",
    "            keep=('messages', 1),  # Keep last 1 messages after summary\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91068064-ef38-4ae3-841b-5bc7be34c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Here is a summary of the conversation to date:\n",
      "\n",
      "## SESSION INTENT\n",
      "ç”¨æˆ·è¯¢é—®å¹¿å·å¤©æ°”åŠå¯»æ±‚é¥®é£Ÿå»ºè®®ï¼Œå¸Œæœ›äº†è§£é¦™èŒ…è¿™ç§é£Ÿæã€‚\n",
      "\n",
      "## SUMMARY\n",
      "ç”¨æˆ·é¦–å…ˆè¯¢é—®å¹¿å·å¤©æ°”ï¼Œå¾—çŸ¥å¤©æ°”å¾ˆå¥½ã€‚æ¥ç€è¯¢é—®åƒä»€ä¹ˆå¥½ï¼ŒAIæ¨èé¦™èŒ…é³—é±¼ç…²ã€‚ç”¨æˆ·ä¸äº†è§£é¦™èŒ…ï¼ŒAIè§£é‡Šé¦™èŒ…åˆåæŸ æª¬è‰ï¼Œå¸¸è§äºæ³°å¼å†¬é˜´åŠŸæ±¤ã€è¶Šå—çƒ¤è‚‰ç­‰ä¸œå—äºšèœè‚´ä¸­ã€‚\n",
      "\n",
      "## ARTIFACTS\n",
      "None\n",
      "\n",
      "## NEXT STEPS\n",
      "None - conversation appears to be casual chat about weather and food recommendations, no specific tasks remain to be completed.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "auv é‚£è¿˜ç­‰ä»€ä¹ˆï¼Œå’±åƒå»å§\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å“ˆå“ˆï¼Œçœ‹ä½ è¿™ä¹ˆå…´å¥‹ï¼ä¸è¿‡æˆ‘å¾—æé†’ä½ ï¼Œä½œä¸ºä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæˆ‘å¯æ²¡æ³•é™ªä½ å‡ºå»åƒå“¦ï½æˆ‘åªèƒ½ç»™ä½ æä¾›ç¾é£Ÿå»ºè®®å’Œä¿¡æ¯ã€‚\n",
      "\n",
      "è¯´åˆ°é¦™èŒ…é³—é±¼ç…²ï¼Œè¿™é“èœç¡®å®å¾ˆé€‚åˆç°åœ¨çš„å¤©æ°”å‘¢ï¼é¦™èŒ…ç‹¬ç‰¹çš„æŸ æª¬é¦™å‘³é…ä¸Šé²œå«©çš„é³—é±¼ï¼Œæƒ³æƒ³å°±æµå£æ°´äº†ï½å¦‚æœä½ çœŸçš„æƒ³å°è¯•çš„è¯ï¼Œå¯ä»¥æ‰¾æ‰¾é™„è¿‘æœ‰æ²¡æœ‰ä¸œå—äºšé£å‘³çš„é¤å…ï¼Œæˆ–è€…å¹²è„†è‡ªå·±åŠ¨æ‰‹åšä¹Ÿè¡Œï¼\n",
      "\n",
      "è¦ä¸æˆ‘å†ç»™ä½ åˆ†äº«ä¸€ä¸‹è¿™é“èœçš„åšæ³•ï¼Ÿå¦‚æœä½ æƒ³è‡ªå·±ä¸‹å¨çš„è¯ï½\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"å¹¿å·ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"å¹¿å·å¤©æ°”å¾ˆå¥½\"},\n",
    "        {\"role\": \"user\", \"content\": \"åƒç‚¹ä»€ä¹ˆå¥½å‘¢\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"è¦ä¸è¦åƒé¦™èŒ…é³—é±¼ç…²\"},\n",
    "        {\"role\": \"user\", \"content\": \"é¦™èŒ…æ˜¯ä»€ä¹ˆ\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"é¦™èŒ…åˆåæŸ æª¬è‰ï¼Œå¸¸è§äºæ³°å¼å†¬é˜´åŠŸæ±¤ã€è¶Šå—çƒ¤è‚‰\"},\n",
    "        {\"role\": \"user\", \"content\": \"auv é‚£è¿˜ç­‰ä»€ä¹ˆï¼Œå’±åƒå»å§\"},\n",
    "    ]},\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725262df-d313-4499-ad09-fe12ac9c7955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
