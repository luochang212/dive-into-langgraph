{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da96a78-5bcd-4ea3-b5cc-addb660a1554",
   "metadata": {},
   "source": [
    "# 中间件\n",
    "\n",
    "[中间件](https://docs.langchain.com/oss/python/langchain/middleware/overview)（middleware）是本次更新中最亮眼的特性，诸多新功能均藉由中间件实现，比如人机交互、动态系统提示词、动态注入上下文等等。可以将中间件理解为一种钩子函数。通过向工作流预埋中间件，可以实现工作流的高效拓展和可定制化。\n",
    "\n",
    "\n",
    "LangChain 支持使用装饰器创建 **自定义中间件**。它提供了以下装饰器：\n",
    "\n",
    "<details>\n",
    "  <summary>装饰器列表</summary>\n",
    "\n",
    "  | DECORATOR | DESCRIPTION |\n",
    "  | -- | -- |\n",
    "  | `@before_agent` | 在 Agent 执行前执行逻辑 |\n",
    "  | `@after_agent` | 在 Agent 执行后执行逻辑 |\n",
    "  | `@before_model` | 在每次模型调用前执行逻辑 |\n",
    "  | `@after_model` | 在每次模型收到响应后执行逻辑 |\n",
    "  | `@wrap_model_call` | 控制模型的调用过程 |\n",
    "  | `@wrap_tool_call` | 控制工具的调用过程 |\n",
    "  | `@dynamic_prompt` | 动态生成系统提示词 |\n",
    "  | `@hook_config` | 配置钩子行为 |\n",
    "\n",
    "</details>\n",
    "\n",
    "**装饰器类型** 决定中间件的执行位置，**被装饰的函数** 实现中间件的具体逻辑。这么说可能有点抽象。没关系，本节提供了四个例子。相信通过这四个例子，你可以感悟到中间件的使用方法：\n",
    "\n",
    "- 预算控制\n",
    "- 消息截断\n",
    "- 敏感词过滤\n",
    "- PII 检测（个人隐私信息检测）\n",
    "\n",
    "## 一、预算控制\n",
    "\n",
    "随着对话轮次的增加，历史对话变得越来越多，每次请求的费用也随之提高。为了控制预算，我们可以设定对话超过某个轮次后，切换到低费率模型。这个功能可以通过自定义中间件实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd14ba5-4b0c-4cfb-a976-040d0a365f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# 加载模型配置\n",
    "_ = load_dotenv()\n",
    "\n",
    "# 低费率模型\n",
    "basic_model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-coder-plus\",\n",
    ")\n",
    "\n",
    "# 高费率模型\n",
    "advanced_model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4033cbf-b59f-4a6b-9fd1-f102b7fea252",
   "metadata": {},
   "source": [
    "具体来说，预算控制功能是由装饰器 [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) 所装饰的函数实现的。我们设定当历史对话超过 5 条后，切换到低费率模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e758792-47d8-4ea4-857e-dddbbc71eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 5:\n",
    "        # Use a basic model for longer conversations\n",
    "        model = basic_model\n",
    "    else:\n",
    "        model = advanced_model\n",
    "\n",
    "    request.model = model\n",
    "    print(f\"message_count: {message_count}\")\n",
    "    print(f\"model_name: {model.model_name}\")\n",
    "\n",
    "    return handler(request)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=advanced_model,  # Default model\n",
    "    middleware=[dynamic_model_selection]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850013a4",
   "metadata": {},
   "source": [
    "通过下面的例子可以看到，在历史对话数 `message_count` 在超过 5 条之后，确实从高费率模型 `qwen3-max` 切换到了低费率模型 `qwen3-coder-plus`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b4ed25-2ef8-4a24-9760-89f1ed34bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 ===\n",
      "message_count: 1\n",
      "model_name: qwen3-max\n",
      "content: 普通汽车有4个轮子。\n",
      "\n",
      "=== Round 2 ===\n",
      "message_count: 3\n",
      "model_name: qwen3-max\n",
      "content: 飞机通常有3个或更多轮子，常见的是2个主轮加1个前轮（共3个）。\n",
      "\n",
      "=== Round 3 ===\n",
      "message_count: 5\n",
      "model_name: qwen3-max\n",
      "content: 摩托车有2个轮子。\n",
      "\n",
      "=== Round 4 ===\n",
      "message_count: 7\n",
      "model_name: qwen3-coder-plus\n",
      "content: 自行车有2个轮子。\n"
     ]
    }
   ],
   "source": [
    "state: MessagesState = {\"messages\": []}\n",
    "items = ['汽车', '飞机', '摩托车', '自行车']\n",
    "for idx, i in enumerate(items):\n",
    "    print(f\"\\n=== Round {idx+1} ===\")\n",
    "    state[\"messages\"] += [HumanMessage(content=f\"{i}有几个轮子，请简单回答\")]\n",
    "    result = agent.invoke(state)\n",
    "    state[\"messages\"] = result[\"messages\"]\n",
    "    print(f\"content: {result[\"messages\"][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc58247-324d-40fe-83e5-b34f9ec564e6",
   "metadata": {},
   "source": [
    "## 二、消息截断\n",
    "\n",
    "智能体系统的上下文是有长度限制的。若超过限制，就应该想办法压缩上下文。在所有方法中，截断是最简单粗暴的。截断功能可以使用 `@before_model` 装饰器实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08619c1-6940-4043-be1b-3282be16c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750eee1-a873-424c-878b-61ec5bf81f0e",
   "metadata": {},
   "source": [
    "现在尝试一种截断策略：除了保留最近的对话之外，额外保留第一条消息。由于我们在第一条消息中告诉智能体「我是 bob」，因此它记得我是 bob.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aefc09f4-d67f-4a7a-b930-171043e0a37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! You introduced yourself earlier.\n"
     ]
    }
   ],
   "source": [
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) <= 3:\n",
    "        return None  # No changes needed\n",
    "\n",
    "    first_msg = messages[0]\n",
    "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *new_messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "agent = create_agent(\n",
    "    basic_model,\n",
    "    middleware=[trim_messages],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "def agent_invoke(agent):\n",
    "    agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "    agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "    agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "    final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "    \n",
    "    final_response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "agent_invoke(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2afa04-18ad-487d-9cef-dfba0b5ec36d",
   "metadata": {},
   "source": [
    "当然，这个表现不足以说明截断中间件真的生效了。若这个中间件从未生效，也会产生这样的结果。为了验证它是否真的生效。我们修改截断策略，改为只保留最后两条对话记录。如果智能体不记得我是 bob，说明截断中间件真的生效了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2a8e62-b7c4-4275-8967-7c551a73c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to your name or personal information. You're interacting with me anonymously. If you'd like to share your name, I'd be happy to use it in our conversation!\n"
     ]
    }
   ],
   "source": [
    "@before_model\n",
    "def trim_without_first_message(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *messages[-2:]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "agent = create_agent(\n",
    "    basic_model,\n",
    "    middleware=[trim_without_first_message],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "agent_invoke(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e989c",
   "metadata": {},
   "source": [
    "看来中间件确实生效了！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1220c4b-13f2-48e0-90d8-3fb2bc92e5bd",
   "metadata": {},
   "source": [
    "## 三、敏感词过滤\n",
    "\n",
    "**护栏**（Guardrails）是智能体提供的一类内容安全能力的统称。我们知道模型本身具有内容风控能力，但很容易被突破。你搜「大模型破甲」就能找到此类教程。智能体可以在模型之外提供额外的安全能力。这是通过工程上的强制性检查实现的。\n",
    "\n",
    "在 LangGraph 中，护栏可以通过中间件轻松实现。下面我们实现一个简单的护栏：若用户最新一条输入中包含指定的敏感词，则智能体拒绝回答此问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c5cc5b-7b4a-4658-8f86-feb8114d04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain.agents.middleware import before_agent, AgentState\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "banned_keywords = [\"hack\", \"exploit\", \"malware\"]\n",
    "\n",
    "@before_agent(can_jump_to=[\"end\"])\n",
    "def content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n",
    "    # Get the first user message\n",
    "    if not state[\"messages\"]:\n",
    "        return None\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.type != \"human\":\n",
    "        return None\n",
    "\n",
    "    content = last_message.content.lower()\n",
    "\n",
    "    # Check for banned keywords\n",
    "    for keyword in banned_keywords:\n",
    "        if keyword in content:\n",
    "            # Block execution before any processing\n",
    "            return {\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n",
    "                }],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "\n",
    "    return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,\n",
    "    middleware=[content_filter],\n",
    ")\n",
    "\n",
    "# This request will be blocked before any processing\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cad2e8-ab78-44d6-8682-1e0fe17469a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How do I hack into a database?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I cannot process requests containing inappropriate content. Please rephrase your request.\n"
     ]
    }
   ],
   "source": [
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761479b-d432-4e09-9341-4b5ad8363b48",
   "metadata": {},
   "source": [
    "## 四、PII 检测\n",
    "\n",
    "接下来，我们继续编写护栏。[PII](https://docs.langchain.com/oss/python/langchain/guardrails#pii-detection)（Personally Identifiable Information）检测可以发现用户输入中的邮箱、IP、地址、银行卡等隐私信息，并做出一些处置。\n",
    "\n",
    "我们尝试两种处置方法：\n",
    "\n",
    "1. 拒绝回答用户问题\n",
    "2. 屏蔽隐私信息：将隐私信息替换为一连串的星号 `********`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0697a36-a7f6-46dc-acf0-44eed2c56daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 可信任的模型，一般是本地模型，为了方便，这里依然使用qwen\n",
    "trusted_model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-coder-plus\",\n",
    ")\n",
    "\n",
    "# 用于格式化智能体输出，若发现敏感信息返回True，没发现返回False\n",
    "class PiiCheck(BaseModel):\n",
    "    \"\"\"Structured output indicating whether text contains PII.\"\"\"\n",
    "    is_pii: bool = Field(description=\"Whether the text contains PII\")\n",
    "\n",
    "def message_with_pii(pii_middleware):\n",
    "    agent = create_agent(\n",
    "        model=basic_model,\n",
    "        middleware=[pii_middleware],\n",
    "    )\n",
    "\n",
    "    # This request will be blocked before any processing\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": dedent(\n",
    "                \"\"\"\n",
    "                File \"/home/luochang/proj/agent.py\", line 53, in my_agent\n",
    "                    agent = create_react_agent(\n",
    "                            ^^^^^^^^^^^^^^^^^^^\n",
    "                File \"/home/luochang/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
    "                    return arg(*args, **kwargs)\n",
    "                        ^^^^^^^^^^^^^^^^^^^^\n",
    "                File \"/home/luochang/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
    "                    model = cast(BaseChatModel, model).bind_tools(\n",
    "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "                AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
    "    \n",
    "                ---\n",
    "    \n",
    "                为啥报错\n",
    "                \"\"\").strip()\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b344fe-5bb5-4349-9183-ed163b5fde4a",
   "metadata": {},
   "source": [
    "**处置方式一**：如遇隐私信息，拒绝回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6400b371-034f-4c71-8dd6-6c5f115a0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@before_agent(can_jump_to=[\"end\"])\n",
    "def content_blocker(state: AgentState,  runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n",
    "    # Get the first user message\n",
    "    if not state[\"messages\"]:\n",
    "        return None\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.type != \"human\":\n",
    "        return None\n",
    "\n",
    "    content = last_message.content.lower()\n",
    "    prompt = (\n",
    "        \"你是一个隐私保护助手。请识别下面文本中涉及个人可识别信息（PII），\"\n",
    "        \"例如：姓名、身份证号、护照号、电话号码、邮箱、住址、银行卡号、社交账号、车牌等。\"\n",
    "        \"特别注意，若代码、文件路径中包含用户名，也应被视为敏感信息。\"\n",
    "        \"若包含敏感信息，请返回{\\\"is_pii\\\": True}，否则返回{\\\"is_pii\\\": False}。\"\n",
    "        \"请严格以 json 格式返回，并且只输出 json。文本如下：\\n\\n\" + content\n",
    "    )\n",
    "\n",
    "    pii_agent = trusted_model.with_structured_output(PiiCheck)\n",
    "    result = pii_agent.invoke(prompt)\n",
    "\n",
    "    if result.is_pii is True:\n",
    "        # Block execution before any processing\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n",
    "            }],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    else:\n",
    "        print(\"No PII found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ba27fa-cb71-4dd2-b051-7ef949d1cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "File \"/home/luochang/proj/agent.py\", line 53, in my_agent\n",
      "    agent = create_react_agent(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
      "    return arg(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
      "    model = cast(BaseChatModel, model).bind_tools(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
      "\n",
      "---\n",
      "\n",
      "为啥报错\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I cannot process requests containing inappropriate content. Please rephrase your request.\n"
     ]
    }
   ],
   "source": [
    "result = message_with_pii(pii_middleware=content_blocker)\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9ad2b-18f9-41e1-995b-81572753f4d2",
   "metadata": {},
   "source": [
    "**处置方式二**：如遇敏感信息，使用一串 `*` 号屏蔽隐私信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a946d8-fde3-4348-82e4-3801481a332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@before_agent(can_jump_to=[\"end\"])\n",
    "def content_filter(state: AgentState,  runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n",
    "    # Get the first user message\n",
    "    if not state[\"messages\"]:\n",
    "        return None\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.type != \"human\":\n",
    "        return None\n",
    "\n",
    "    content = last_message.content.lower()\n",
    "    prompt = (\n",
    "        \"你是一个隐私保护助手。请识别下面文本中涉及个人可识别信息（PII），\"\n",
    "        \"例如：姓名、身份证号、护照号、电话号码、邮箱、住址、银行卡号、社交账号、车牌等。\"\n",
    "        \"特别注意，若代码、文件路径中包含用户名，也应被视为敏感信息。\"\n",
    "        \"若包含敏感信息，请返回{\\\"is_pii\\\": True}，否则返回{\\\"is_pii\\\": False}。\"\n",
    "        \"请严格以 json 格式返回，并且只输出 json。文本如下：\\n\\n\" + content\n",
    "    )\n",
    "\n",
    "    pii_agent = trusted_model.with_structured_output(PiiCheck)\n",
    "    result = pii_agent.invoke(prompt)\n",
    "\n",
    "    if result.is_pii is True:\n",
    "        mask_prompt = (\n",
    "            \"你是一个隐私保护助手。请将下面文本中的所有个人可识别信息（PII）用星号（*）替换。\"\n",
    "            \"仅替换敏感片段，其他文本保持不变。\"\n",
    "            \"只输出处理后的文本，不要任何解释或额外内容。文本如下：\\n\\n\" + last_message.content\n",
    "        )\n",
    "        masked_message = basic_model.invoke(mask_prompt)\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": masked_message.content\n",
    "            }]\n",
    "        }\n",
    "    else:\n",
    "        print(\"No PII found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2480e24d-39b9-4329-a008-6b162850f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "File \"/home/luochang/proj/agent.py\", line 53, in my_agent\n",
      "    agent = create_react_agent(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
      "    return arg(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
      "    model = cast(BaseChatModel, model).bind_tools(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
      "\n",
      "---\n",
      "\n",
      "为啥报错\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "File \"/home/********/proj/agent.py\", line 53, in my_agent\n",
      "    agent = create_react_agent(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/********/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
      "    return arg(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/********/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
      "    model = cast(BaseChatModel, model).bind_tools(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
      "\n",
      "---\n",
      "\n",
      "为啥报错\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "这个错误的原因是 `create_react_agent` 函数期望接收一个 `BaseChatModel` 类型的模型对象，但你传入的是一个 `RunnableLambda` 对象。\n",
      "\n",
      "## 错误分析\n",
      "\n",
      "从错误信息可以看出：\n",
      "1. `create_react_agent` 在第 566 行尝试调用 `model.bind_tools()`\n",
      "2. 但它发现传入的 `model` 是 `RunnableLambda` 类型，没有 `bind_tools` 方法\n",
      "3. 只有继承自 `BaseChatModel` 的模型（如 ChatOpenAI、ChatAnthropic 等）才有 `bind_tools` 方法\n",
      "\n",
      "## 解决方案\n",
      "\n",
      "### 方案1：使用正确的聊天模型\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langgraph.prebuilt import create_react_agent\n",
      "\n",
      "# 使用实际的聊天模型\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "agent = create_react_agent(llm, tools)\n",
      "```\n",
      "\n",
      "### 方案2：如果你需要使用 RunnableLambda\n",
      "如果你想在模型前后添加自定义逻辑，可以这样做：\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
      "\n",
      "# 创建基础模型\n",
      "base_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "\n",
      "# 如果需要添加预处理逻辑\n",
      "def preprocess(input_data):\n",
      "    # 你的预处理逻辑\n",
      "    return input_data\n",
      "\n",
      "preprocessor = RunnableLambda(preprocess)\n",
      "\n",
      "# 组合模型\n",
      "enhanced_model = preprocessor | base_model\n",
      "\n",
      "# 但要注意，create_react_agent 需要原始的 chat model 来绑定工具\n",
      "# 所以你应该直接传递 base_model 给 create_react_agent\n",
      "agent = create_react_agent(base_model, tools)\n",
      "```\n",
      "\n",
      "### 方案3：检查你的代码\n",
      "确保你在调用 `create_react_agent` 时传入的是正确的模型：\n",
      "\n",
      "```python\n",
      "# ❌ 错误 - 传入了 RunnableLambda\n",
      "model = RunnableLambda(some_function)\n",
      "agent = create_react_agent(model, tools)  # 这会报错\n",
      "\n",
      "# ✅ 正确 - 传入 ChatModel\n",
      "from langchain_openai import ChatOpenAI\n",
      "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
      "agent = create_react_agent(model, tools)  # 这样正确\n",
      "```\n",
      "\n",
      "## 总结\n",
      "\n",
      "你需要将一个真正的聊天模型（如 `ChatOpenAI`、`ChatAnthropic` 等）传递给 `create_react_agent`，而不是 `RunnableLambda`。如果需要自定义逻辑，可以在模型外部处理，或者确保最终传递给 `create_react_agent` 的是支持 `bind_tools` 方法的聊天模型实例。\n"
     ]
    }
   ],
   "source": [
    "result = message_with_pii(pii_middleware=content_filter)\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d7771-f9b8-4408-9387-c67c51c1a155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3.13)",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
