"""
ä¸€ä¸ªæ™ºèƒ½ä½“
"""

import os
import asyncio
import textwrap

from typing import List, Dict
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents.middleware import SummarizationMiddleware, TodoListMiddleware, dynamic_prompt, ModelRequest
from utils.web_ui import create_ui, theme, custom_css
from utils.tool_result import format_tool_result
from utils.fix_deepseek import DeepSeekChatOpenAI
from tools.tool_math import add, subtract, multiply, divide
from tools.tool_search import dashscope_search, SearchTool
from prompts.prompt_base import get_system_prompt_base
from prompts.prompt_enhance import get_system_prompt_enhance


# åŠ è½½æ¨¡å‹é…ç½®
# è¯·äº‹å…ˆåœ¨ .env ä¸­é…ç½® DASHSCOPE_API_KEY
load_dotenv()


# å…¨å±€å˜é‡
_agent = None  # å…¨å±€ Agent å®ä¾‹
_llm = None  # å…¨å±€ LLM å®ä¾‹
_greeting = ""  # æ™ºèƒ½ä½“è‡ªæˆ‘ä»‹ç»


@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    return get_system_prompt_enhance()


async def get_agent():
    """è·å–å…¨å±€ Agent å®ä¾‹"""
    global _agent, _llm
    if _agent is None:
        # ä½¿ç”¨ DashScope
        llm = ChatOpenAI(
            # é˜¿é‡Œ DashScope ç›®å‰æœ‰å…è´¹é¢åº¦ï¼Œæ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š
            # kimi-k2-thinking / deepseek-v3.2 / glm-4.7 / qwen3-coder-plus-2025-07-22
            # å¦‚æœè§‰å¾—å¡ï¼Œå¯ä»¥ä½¿ç”¨ä»˜è´¹æ¨¡å‹ï¼š
            # qwen3-coder-plus / qwen3-max / qwen3-max-preview
            model="qwen3-max",
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url=os.getenv("DASHSCOPE_BASE_URL"),
            max_retries=3,
            timeout=30,
        )

        # # ä½¿ç”¨ DeepSeekï¼ˆä¸æ¨èï¼Œè°ƒç”¨ MCP ç»å¸¸æœ‰é—®é¢˜ï¼‰
        # llm = DeepSeekChatOpenAI(
        #     model="deepseek-chat",  # deepseek-chat / deepseek-reasoner
        #     api_key=os.getenv("DEEPSEEK_API_KEY"),
        #     base_url=os.getenv("DEEPSEEK_BASE_URL"),
        #     max_retries=3,
        #     timeout=30,
        # )

        _llm = llm

        # æ¥å…¥ MCP
        client = MultiServerMCPClient(  
            {
                # ä¸‹é¢æ ‡ ğŸŒŸ çš„æœåŠ¡å»ºè®®å¼€å¯
                # ============= è§’è‰²æ‰®æ¼” MCP =============
                # ğŸŒŸ stdio
                "role-play": {
                    "command": "python",
                    "args": [os.path.abspath("./mcp/role_play.py")],
                    "transport": "stdio",
                },
                # # streamable http
                # "role-play": {
                #     "url": "http://localhost:8000/mcp",
                #     "transport": "streamable_http",
                # },
                # ============= ä»£ç æ‰§è¡Œ MCP =============
                # ğŸŒŸ stdio
                "code-execution": {
                    "command": "python",
                    "args": [os.path.abspath("./mcp/code_execution.py")],
                    "transport": "stdio",
                },
                # # streamable http
                # "code-execution": {
                #     "url": "http://localhost:8001/mcp",
                #     "transport": "streamable_http",
                # },
                # ============= é«˜å¾·åœ°å›¾ MCP =============
                # # ğŸŒŸ streamable http
                # # å¿…é¡»å…ˆç”³è¯·é«˜å¾·åœ°å›¾ API_KEYï¼Œè¯¦è§ .env.example
                # "é«˜å¾·åœ°å›¾": {
                #     "url": f"https://mcp.amap.com/mcp?key={os.getenv('AMAP_API_KEY')}",
                #     "transport": "streamable_http",
                # },
                # ============= å›¾è¡¨å¯è§†åŒ– MCP =============
                # # stdio
                # "å›¾è¡¨å¯è§†åŒ–": {
                #     "command": "npx",
                #     "args": ["-y", "@antv/mcp-server-chart"],
                # },
                # # ğŸŒŸ streamable http
                # # å¿…é¡»å…ˆå¯åŠ¨æœåŠ¡ï¼Œå‚è€ƒ mcp/mcp-server-chart/README.md
                # "å›¾è¡¨å¯è§†åŒ–": {
                #     "url": "http://localhost:1123/mcp",
                #     "transport": "streamable_http",
                # },
                # # ============= æ–‡ä»¶ç³»ç»Ÿ MCP =============
                # # stdio
                # "filesystem": {
                #     "command": "npx",
                #     "args": [
                #         "-y",
                #         "@modelcontextprotocol/server-filesystem",
                #         os.path.abspath("./space/"),
                #     ]
                # },
            }
        )
        mcp_tools = await client.get_tools()

        # åˆ›å»º subagent
        @tool(
            "subagent",
            description="é€šç”¨å­æ™ºèƒ½ä½“ (subagent)ï¼Œæ‹¥æœ‰ç‹¬ç«‹ä¸Šä¸‹æ–‡ç©ºé—´ï¼Œæ”¯æŒè”ç½‘æœç´¢"
        )
        def call_subagent(query: str):
            subagent = create_agent(
                model=llm,
                tools=[dashscope_search],
                name="subagent",
            )
            result = subagent.invoke({
                "messages": [{"role": "user", "content": query}]
            })
            return result["messages"][-1].content

        # åˆ›å»ºæ™ºèƒ½ä½“
        _agent = create_agent(
            model=llm,
            tools=mcp_tools + [add, subtract, multiply, divide, dashscope_search, call_subagent],
            system_prompt=get_system_prompt_base(),
            middleware=[
                dynamic_system_prompt,
                SummarizationMiddleware(
                    model=llm,
                    trigger=("tokens", 2000),
                    keep=("messages", 7),
                ),
                TodoListMiddleware(
                    system_prompt="\n".join([
                        "å½“ç”¨æˆ·è¯·æ±‚è¾ƒä¸ºå¤æ‚ä¸”å¯ä»¥æ‹†åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡æ—¶ï¼Œåˆ›å»ºä»»åŠ¡åˆ—è¡¨ã€‚",
                        "å½“ç”¨æˆ·ä¸»åŠ¨è¦æ±‚åˆ›å»ºä»»åŠ¡åˆ—è¡¨ (todo list) æ—¶ï¼Œå¿…é¡»åˆ›å»ºä»»åŠ¡åˆ—è¡¨ã€‚",
                        "\nä»¥ä¸‹ 3 ç§æƒ…å½¢æ— éœ€åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼š",
                        "1. ä»»åŠ¡è¿‡äºç®€å•æ—¶ï¼Œæ— éœ€åˆ›å»º",
                        "2. ä»»åŠ¡æ•°é‡å°äº 3 ä¸ªæ—¶ï¼Œæ— éœ€åˆ›å»º",
                        "3. çº¯æ–‡å­—åˆ†æã€æ— å·¥å…·è°ƒç”¨æ—¶ï¼Œæ— éœ€åˆ›å»º",
                        "\nä½¿ç”¨ write_todos å·¥å…·ç®¡ç†ä»»åŠ¡åˆ—è¡¨æ—¶ï¼Œéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š",
                        "1. ä»»åŠ¡åˆ†è§£ï¼šåº”æ»¡è¶³â€œä½è€¦åˆï¼Œé«˜å†…èšâ€çš„åŸåˆ™",
                        "2. å‰ç½®ä»»åŠ¡ï¼šç¡®ä¿å½“å‰ä»»åŠ¡çš„å‰ç½®ä¾èµ–å·²å®Œæˆï¼ˆå¦‚æœ‰ï¼‰",
                        "3. å®Œæˆæ ‡å‡†ï¼šæ¯ä¸ªä»»åŠ¡åº”è¯¥æœ‰æ˜ç¡®çš„éªŒæ”¶æ ‡å‡†",
                        "4. çŠ¶æ€æµè½¬ï¼šåœ¨ä»»åŠ¡çŠ¶æ€æ”¹å˜æ—¶ç«‹å³æ›´æ–°ï¼ˆå¾…åŠ/è¿›è¡Œä¸­/å®Œæˆ/å–æ¶ˆï¼‰",
                        "\næ¯å®Œæˆ 1 ä¸ªä»»åŠ¡ï¼Œç”¨ Markdown è¡¨æ ¼å‘ç”¨æˆ·å±•ç¤ºå½“å‰ä»»åŠ¡åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š",
                        "| ID | ä»»åŠ¡ | çŠ¶æ€ | ",
                        "| -- | -- | -- |",
                        "| 1 | ä»»åŠ¡1 | å®Œæˆ |",
                        "| 2 | ä»»åŠ¡2 | è¿›è¡Œä¸­ |",
                        "| 3 | ä»»åŠ¡3 | å¾…åŠ |",
                    ])
                ),
            ],
        )
    return _agent


def get_tools():
    """è·å– Agent çš„å·¥å…·åˆ—è¡¨"""
    agent = asyncio.run(get_agent())
    node = agent.get_graph().nodes["tools"]
    tools = list(node.data.tools_by_name.values())

    # ä¼˜åŒ–å·¥å…·å±•ç¤º
    if len(tools) > 12:
        # å½“å·¥å…·è¿‡å¤šæ—¶ï¼Œä»…æ˜¾ç¤ºå·¥å…·å
        tool_names = [tool.name for tool in tools]
        wrapped_text = textwrap.fill(" / ".join(tool_names), width=110)
        return f"\n```text\n{wrapped_text}\n```\n"
    else:
        # å½“å·¥å…·ä¸å¤šæ—¶ï¼Œæ˜¾ç¤ºå·¥å…·æè¿°
        lines = []
        for tool in tools:
            desc = (tool.description or "").split('\n')[0]
            lines.append(f"- `{tool.name}`: {desc}")
        return "\n".join(lines)


def get_greeting():
    """è·å– Agent çš„è‡ªæˆ‘ä»‹ç»"""
    global _greeting
    if not _greeting:
        try:
            tools_info = get_tools()
            _greeting = "\n".join([
                "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨çš„å·¥å…·åŒ…æ‹¬ï¼š",
                tools_info,
                "\nè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ",
            ])
        except Exception as e:
            print(f"è·å–å·¥å…·åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            _greeting = "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ã€‚\nè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
    return _greeting


async def generate_response(message: str,
                            history: List[Dict[str, str]]
):
    """ç”Ÿæˆ Agent çš„å“åº”"""
    if not message:
        yield "", history
        return

    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": ""})

    messages = history[:-1]

    agent = await get_agent()

    # é¿å… MCP è°ƒç”¨å¤±è´¥å¼•å‘çš„é€€å‡º
    try:
        async for token, metadata in agent.astream(
            {"messages": messages},
            stream_mode="messages",
            context=SearchTool(api_key=os.getenv("DASHSCOPE_API_KEY")),
        ):
            if metadata["langgraph_node"] == "model":
                content = token.content_blocks
                if content and content[0].get("text", "") != "":
                    history[-1]["content"] += content[0]["text"]
                    yield "", history
            elif metadata["langgraph_node"] == "tools":
                content = token.content_blocks
                if content and content[0].get("text", "") != "":
                    tool_name = token.name
                    tool_output = content[0]["text"]
                    # Format the tool output
                    formatted_output = format_tool_result(tool_name, tool_output)
                    history[-1]["content"] += formatted_output
                    yield "", history
    except Exception as err:
        print(f"å‘ç”Ÿé”™è¯¯: {err}")

        # ä¼˜å…ˆè¾“å‡ºæ—¥å¿—æ‘˜è¦ï¼Œå¦åˆ™é™çº§è¾“å‡ºåŸæ—¥å¿—
        summary = ""
        try:
            abstract = _llm.invoke("\n".join([
                str(err),
                "---",
                "ä»¥ä¸Šæ˜¯ LangChain Agent çš„æŠ¥é”™ä¿¡æ¯ï¼Œè¯·ç®€è¿°æŠ¥é”™åŸå› ï¼š",
            ]))
            summary = f"\nâš ï¸ å‘ç”Ÿé”™è¯¯ï¼Œä»¥ä¸‹æ˜¯æ‘˜è¦ä¿¡æ¯ï¼š\n{abstract}"
        except Exception:
            summary = f"\nâš ï¸ å‘ç”Ÿé”™è¯¯ï¼Œä»¥ä¸‹æ˜¯åŸæ—¥å¿—ï¼š\n{str(err)[:300]}"

        history[-1]["content"] += summary
        yield "", history

    yield "", history


def main():
    """ä¸»å‡½æ•°"""
    app = create_ui(
        llm_func=generate_response,
        tab_name="Gradio APP - WebUI",
        main_title="Gradio Agent APP",
        initial_message=[{"role": "assistant", "content": get_greeting()}]
    )

    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        theme=theme,
        css=custom_css
    )


if __name__ == "__main__":
    main()
