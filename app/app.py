"""
ä¸€ä¸ªæ™ºèƒ½ä½“
"""

import os
import asyncio
import textwrap
import argparse

from typing import List, Dict, AsyncIterator, Tuple
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents.middleware import SummarizationMiddleware, TodoListMiddleware, dynamic_prompt, ModelRequest
from utils.web_ui import create_ui, theme, custom_css
from utils.tool_view import format_tool_call, format_tool_result
from utils.remove_html import get_cleaned_text
from tools.tool_math import add, subtract, multiply, divide
from tools.tool_search import dashscope_search, SearchTool
from prompts.prompt_base import get_system_prompt_base
from prompts.prompt_enhance import get_system_prompt_enhance


# åŠ è½½æ¨¡å‹é…ç½®
# æ³¨æ„â€¼ï¸ï¼šè¯·å…ˆåœ¨ .env ä¸­é…ç½® DASHSCOPE_API_KEY
load_dotenv()


# æ˜¯å¦æ¸…æ´—å†å²å¯¹è¯è®°å½•ä¸­çš„ HTML å†…å®¹
REMOVE_HTML = False


# å…¨å±€å˜é‡
_agent = None  # å…¨å±€ Agent å®ä¾‹
_llm = None  # å…¨å±€ LLM å®ä¾‹
_greeting = ""  # æ™ºèƒ½ä½“è‡ªæˆ‘ä»‹ç»


@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    return get_system_prompt_enhance()


async def get_agent():
    """è·å–å…¨å±€ Agent å®ä¾‹"""
    global _agent, _llm
    if _agent is None:
        # ==================== ä½¿ç”¨ DashScope ====================
        # é˜¿é‡Œ DashScope ç›®å‰æœ‰å…è´¹é¢åº¦ï¼Œæ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š
        #   kimi-k2-thinking / deepseek-v3.2 / glm-4.7 / qwen3-coder-plus-2025-07-22
        # å¦‚æœè§‰å¾—å¡ï¼Œå¯ä»¥ä½¿ç”¨ä»˜è´¹æ¨¡å‹ï¼š
        #   qwen3-max / qwen3-max-preview / qwen3-coder-plus
        llm = ChatOpenAI(
            model="qwen3-coder-plus",
            base_url=os.getenv("DASHSCOPE_BASE_URL"),
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            timeout=30,
            extra_body={
                "chat_template_kwargs": {
                    "enable_thinking": True,
                }
            }
        )
        # ==================== ä½¿ç”¨ Ark ====================
        # # å­—èŠ‚ç«å±±æ–¹èˆŸ ç›®å‰æœ‰å…è´¹é¢åº¦ï¼Œæ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š
        # #   deepseek-v3-2-251201 / kimi-k2-thinking-251104 / doubao-seed-1-8-251228
        # llm = ChatOpenAI(
        #     model="deepseek-v3-2-251201",
        #     base_url=os.getenv("ARK_BASE_URL"),
        #     api_key=os.getenv("ARK_API_KEY"),
        #     max_retries=1,
        #     timeout=30,
        # )
        # ==================== ä½¿ç”¨ Ollama ====================
        # # ä½¿ç”¨å‰éœ€è¦ï¼š
        # # 1. ä¸‹è½½ qwen3:4b æ¨¡å‹ 
        # #     ollama pull qwen3:4b
        # # 2. å¼€å¯ Ollama æœåŠ¡
        # #     OLLAMA_HOST=0.0.0.0:11435 ollama serve
        # llm = ChatOpenAI(
        #     model="qwen3:4b",
        #     base_url="http://127.0.0.1:11435/v1",
        #     api_key="-",  # éç©º
        #     max_retries=1,
        #     timeout=30,
        #     temperature=0.7,
        #     top_p=0.9,
        #     extra_body={
        #         "chat_template_kwargs": {
        #             "enable_thinking": True,
        #         }
        #     }
        # )
        # ==================== The End ====================

        _llm = llm

        # æ¥å…¥ MCP
        client = MultiServerMCPClient(  
            {
                # ä¸‹é¢æ ‡ ğŸŒŸ çš„æœåŠ¡å»ºè®®å¼€å¯
                # =============== è§’è‰²æ‰®æ¼” MCP ===============
                # ğŸŒŸ stdio
                "role-play": {
                    "command": "python",
                    "args": [os.path.abspath("./mcp/role_play.py")],
                    "transport": "stdio",
                },
                # # streamable http
                # "role-play": {
                #     "url": "http://localhost:8000/mcp",
                #     "transport": "streamable_http",
                # },
                # =============== ä»£ç æ‰§è¡Œ MCP ===============
                # ğŸŒŸ stdio
                "code-execution": {
                    "command": "python",
                    "args": [os.path.abspath("./mcp/code_execution.py")],
                    "transport": "stdio",
                },
                # # streamable http
                # "code-execution": {
                #     "url": "http://localhost:8001/mcp",
                #     "transport": "streamable_http",
                # },
                # =============== é«˜å¾·åœ°å›¾ MCP ===============
                # ğŸŒŸ streamable http
                # å¿…é¡»å…ˆç”³è¯·é«˜å¾·åœ°å›¾ API_KEYï¼Œè¯¦è§ .env.example
                "é«˜å¾·åœ°å›¾": {
                    "url": f"https://mcp.amap.com/mcp?key={os.getenv('AMAP_API_KEY')}",
                    "transport": "streamable_http",
                },
                # =============== å›¾è¡¨å¯è§†åŒ– MCP ===============
                # # ğŸŒŸ stdio
                # "å›¾è¡¨å¯è§†åŒ–": {
                #     "command": "npx",
                #     "args": ["-y", "@antv/mcp-server-chart"],
                #     "transport": "stdio",
                # },
                # # streamable http
                # # å¿…é¡»å…ˆå¯åŠ¨æœåŠ¡ï¼Œå‚è€ƒ mcp/mcp-server-chart/README.md
                # "å›¾è¡¨å¯è§†åŒ–": {
                #     "url": "http://localhost:1123/mcp",
                #     "transport": "streamable_http",
                # },
                # =============== æ–‡ä»¶ç³»ç»Ÿ MCP ===============
                # # ğŸŒŸ stdio
                # "filesystem": {
                #     "command": "npx",
                #     "args": [
                #         "-y",
                #         "@modelcontextprotocol/server-filesystem",
                #         os.path.abspath("./space"),
                #     ],
                #     "transport": "stdio",
                # },
                # =============== The End ===============
            }
        )
        mcp_tools = await client.get_tools()

        # åˆ›å»º subagent
        @tool(
            "subagent",
            description="é€šç”¨å­æ™ºèƒ½ä½“ (subagent)ï¼Œæ‹¥æœ‰ç‹¬ç«‹ä¸Šä¸‹æ–‡ç©ºé—´ï¼Œæ”¯æŒè”ç½‘æœç´¢"
        )
        def call_subagent(query: str):
            subagent = create_agent(
                model=llm,
                tools=[dashscope_search],
                name="subagent",
            )
            result = subagent.invoke({
                "messages": [{"role": "user", "content": query}]
            })
            return result["messages"][-1].content

        # åˆ›å»ºæ™ºèƒ½ä½“
        _agent = create_agent(
            model=llm,
            tools=mcp_tools + [add, subtract, multiply, divide, dashscope_search, call_subagent],
            system_prompt=get_system_prompt_base(),
            middleware=[
                dynamic_system_prompt,
                SummarizationMiddleware(
                    model=llm,
                    trigger=("tokens", 2000),
                    keep=("messages", 7),
                ),
                TodoListMiddleware(
                    system_prompt="\n".join([
                        "å½“ç”¨æˆ·è¯·æ±‚è¾ƒä¸ºå¤æ‚ä¸”å¯ä»¥æ‹†åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡æ—¶ï¼Œåˆ›å»ºä»»åŠ¡åˆ—è¡¨ã€‚",
                        "å½“ç”¨æˆ·ä¸»åŠ¨è¦æ±‚åˆ›å»ºä»»åŠ¡åˆ—è¡¨ (todo list) æ—¶ï¼Œå¿…é¡»åˆ›å»ºä»»åŠ¡åˆ—è¡¨ã€‚",
                        "\nä»¥ä¸‹ 3 ç§æƒ…å½¢æ— éœ€åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼š",
                        "1. ä»»åŠ¡è¿‡äºç®€å•æ—¶ï¼Œæ— éœ€åˆ›å»º",
                        "2. ä»»åŠ¡æ•°é‡å°äº 3 ä¸ªæ—¶ï¼Œæ— éœ€åˆ›å»º",
                        "3. çº¯æ–‡å­—åˆ†æã€æ— å·¥å…·è°ƒç”¨æ—¶ï¼Œæ— éœ€åˆ›å»º",
                        "\nä½¿ç”¨ write_todos å·¥å…·ç®¡ç†ä»»åŠ¡åˆ—è¡¨æ—¶ï¼Œéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š",
                        "1. ä»»åŠ¡åˆ†è§£ï¼šåº”æ»¡è¶³â€œä½è€¦åˆï¼Œé«˜å†…èšâ€çš„åŸåˆ™",
                        "2. å‰ç½®ä»»åŠ¡ï¼šç¡®ä¿å½“å‰ä»»åŠ¡çš„å‰ç½®ä¾èµ–å·²å®Œæˆï¼ˆå¦‚æœ‰ï¼‰",
                        "3. å®Œæˆæ ‡å‡†ï¼šæ¯ä¸ªä»»åŠ¡åº”è¯¥æœ‰æ˜ç¡®çš„éªŒæ”¶æ ‡å‡†",
                        "4. çŠ¶æ€æµè½¬ï¼šåœ¨ä»»åŠ¡çŠ¶æ€æ”¹å˜æ—¶ç«‹å³æ›´æ–°ï¼ˆå¾…åŠ/è¿›è¡Œä¸­/å®Œæˆ/å–æ¶ˆï¼‰",
                        "\næ¯å®Œæˆ 1 ä¸ªä»»åŠ¡ï¼Œç”¨ Markdown è¡¨æ ¼å‘ç”¨æˆ·å±•ç¤ºå½“å‰ä»»åŠ¡åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š",
                        "| ID | ä»»åŠ¡ | çŠ¶æ€ | ",
                        "| -- | -- | -- |",
                        "| 1 | ä»»åŠ¡1 | å®Œæˆ |",
                        "| 2 | ä»»åŠ¡2 | è¿›è¡Œä¸­ |",
                        "| 3 | ä»»åŠ¡3 | å¾…åŠ |",
                    ])
                ),
            ],
        )
    return _agent


def get_tools():
    """è·å– Agent çš„å·¥å…·åˆ—è¡¨"""
    agent = asyncio.run(get_agent())
    node = agent.get_graph().nodes["tools"]
    tools = list(node.data.tools_by_name.values())

    # ä¼˜åŒ–å·¥å…·å±•ç¤º
    if len(tools) < 13:
        # å½“å·¥å…·ä¸å¤šæ—¶ï¼Œå±•ç¤ºå·¥å…·æè¿°
        lines = []
        for tool in tools:
            desc = (tool.description or "").split('\n')[0]
            lines.append(f"- `{tool.name}`: {desc}")
        return "\n".join(lines)
    else:
        # å½“å·¥å…·è¿‡å¤šæ—¶ï¼Œä»…å±•ç¤ºå·¥å…·åç§°
        tool_names = [tool.name for tool in tools]
        wrapped_text = textwrap.fill(" / ".join(tool_names), width=110)
        return f"\n```text\n{wrapped_text}\n```\n"


def get_greeting():
    """è·å– Agent çš„è‡ªæˆ‘ä»‹ç»"""
    global _greeting
    if not _greeting:
        try:
            tools_info = get_tools()
            _greeting = "\n".join([
                "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨çš„å·¥å…·åŒ…æ‹¬ï¼š",
                tools_info,
                "\nè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ",
            ])
        except Exception as e:
            print(f"è·å–å·¥å…·åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            _greeting = "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ã€‚\nè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
    return _greeting


def err_summary(err: Exception, limit: int = 300) -> str:
    """æ€»ç»“ Agent è¿è¡Œé”™è¯¯"""
    # ä¼˜å…ˆè¾“å‡ºæ—¥å¿—æ‘˜è¦ï¼Œå¤±è´¥åˆ™é™çº§ä¸ºåŸå§‹æ—¥å¿—
    summary = ""
    try:
        abstract = _llm.invoke("\n".join([
            str(err),
            "---",
            "ä»¥ä¸Šæ˜¯ LangChain Agent çš„æŠ¥é”™ä¿¡æ¯ï¼Œè¯·ç®€è¿°æŠ¥é”™åŸå› ï¼š",
        ]))
        summary = f"\n âš ï¸ å‘ç”Ÿé”™è¯¯ï¼Œä»¥ä¸‹æ˜¯æ‘˜è¦ä¿¡æ¯ï¼š\n{abstract}"
    except Exception:
        summary = f"\n âš ï¸ å‘ç”Ÿé”™è¯¯ï¼Œä»¥ä¸‹æ˜¯åŸå§‹æ—¥å¿—ï¼š\n{str(err)[:limit]}"

    return summary


async def _agent_events(
    agent,
    messages,
    history: List[Dict[str, str]],
) -> AsyncIterator[Tuple[str, List[Dict[str, str]]]]:
    """ç®€åŒ–æ˜¾ç¤ºï¼Œä»…å¤„ç† messages äº‹ä»¶æµ"""
    async for token, metadata in agent.astream(
        {"messages": messages},
        stream_mode="messages",
        context=SearchTool(api_key=os.getenv("DASHSCOPE_API_KEY")),
    ):
        if metadata["langgraph_node"] == "model":
            # æ¨¡å‹å›å¤
            if token.content:
                history[-1]["content"] += token.content
                yield "", history
        elif metadata["langgraph_node"] == "tools":
            # å·¥å…·è°ƒç”¨ç»“æœ
            if token.content:
                history[-1]["content"] += format_tool_result(token.name, token.content)
                yield "", history


async def _agent_events_optimize(
    agent,
    messages,
    history: List[Dict[str, str]],
) -> AsyncIterator[Tuple[str, List[Dict[str, str]]]]:
    """ä¼˜åŒ–æ˜¾ç¤ºï¼Œå¤„ç† messages å’Œ values äº‹ä»¶æµ"""
    async for mode, payload in agent.astream(
        {"messages": messages},
        stream_mode=["messages", "values"],
        context=SearchTool(api_key=os.getenv("DASHSCOPE_API_KEY")),
    ):
        if mode == "messages":
            token, metadata = payload
            current_node = metadata["langgraph_node"]
            if current_node == "model":
                # æ¨¡å‹å›å¤
                if token.content:
                    history[-1]["content"] += token.content
                    yield "", history
            elif current_node == "tools":
                # å·¥å…·è°ƒç”¨ç»“æœ
                if token.content:
                    history[-1]["content"] += format_tool_result(token.name, token.content)
                    yield "", history
        elif mode == "values":
            state = payload
            state_messages = state.get("messages") if isinstance(state, dict) else None
            if not state_messages:
                continue
            last_message = state_messages[-1]

            # å·¥å…·è°ƒç”¨å…¥å‚
            tool_calls = getattr(last_message, "tool_calls", None)
            if tool_calls:
                history[-1]["content"] += "".join(
                    format_tool_call((tc.get("name") or "unknown"), (tc.get("args") or {}))
                    for tc in tool_calls
                )
                yield "", history


async def generate_response(message: str,
                            history: List[Dict[str, str]]
):
    """ç”Ÿæˆ Agent çš„å“åº”"""
    if not message:
        yield "", history
        return

    # æ¸…æ´—ä¸Šä¸€æ¡ AI å›å¤ä¸­çš„ html å†…å®¹ï¼Œå¯ä»¥ä¸ºä¸Šä¸‹æ–‡å‡è´Ÿ
    # å¦‚æœä¸åŠ å·¥å…·æ¶ˆæ¯å’Œæ€ç»´é“¾æ¶ˆæ¯å°†æ²¡æœ‰è¿™ä¹ˆå¤šäº‹(`ãƒ®Â´ )
    if REMOVE_HTML and len(history) >= 1 and history[-1]["role"] == "assistant":
        html_content = history[-1]["content"][0]['text']
        history[-1]["content"][0]['text'] = get_cleaned_text(html_content)

    # print("=================================")
    # print(history)

    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": ""})

    messages = history[:-1]

    agent = await get_agent()

    # é¿å… MCP è°ƒç”¨å¤±è´¥å¼•å‘çš„é€€å‡º
    try:
        # ä½¿ç”¨ä¼˜åŒ–æ˜¾ç¤º
        async for update in _agent_events_optimize(agent, messages, history):
            yield update
    except Exception as err:
        print(f"å‘ç”Ÿé”™è¯¯: {err}")
        history[-1]["content"] += err_summary(err)
        yield "", history

    yield "", history


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®ç½‘ç»œå‚æ•°
    # ä¸º docker é¢„ç•™çš„æ“ä½œå…¥å£ï¼Œå› ä¸º docker çš„ host ä¸€èˆ¬è®¾ç½®ä¸º 0.0.0.0
    parser = argparse.ArgumentParser(description="Gradio Agent APP")
    parser.add_argument("--host", type=str, default="localhost", help="ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=7860, help="ç«¯å£å·")
    args = parser.parse_args()

    app = create_ui(
        llm_func=generate_response,
        tab_name="Gradio APP - WebUI",
        main_title="Gradio Agent APP",
        initial_message=[{"role": "assistant", "content": get_greeting()}]
    )

    app.launch(
        server_name=args.host,
        server_port=args.port,
        share=False,
        theme=theme,
        css=custom_css
    )


if __name__ == "__main__":
    main()
